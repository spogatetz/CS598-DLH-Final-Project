{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n",
      "857\n",
      "857\n"
     ]
    }
   ],
   "source": [
    "with open('./learning_data/spo_train_cleaned_1.txt', 'r', encoding=\"latin-1\") as f:\n",
    "    first_list_train = []\n",
    "    fourth_list_train = []\n",
    "    fifth_list_train = []\n",
    "    for line in f:\n",
    "        row = line.strip().split('\\t')\n",
    "        if (row[3] is not None) and (row[4] is not None):\n",
    "            row_3 = eval(row[3])\n",
    "            row_4 = eval(row[4])\n",
    "            if (row_3 is not None) and (row_4 is not None):\n",
    "                if (len(row_3) == 3) and (len(row_4) == 3) and (None not in (row_3)) and (None not in (row_4)):\n",
    "                    lower_row_3 = []\n",
    "                    for i in row_3:\n",
    "                        lower_row_3.append(i.lower())\n",
    "                    lower_row_4 = []\n",
    "                    for i in row_4:\n",
    "                        lower_row_4.append(i.lower())\n",
    "                    first_list_train.append(int(row[0]))\n",
    "                    fourth_list_train.append(lower_row_3)\n",
    "                    fifth_list_train.append(lower_row_4) \n",
    "\n",
    "with open('./learning_data/spo_test_cleaned_1.txt', 'r', encoding=\"latin-1\") as f:\n",
    "    first_list_test = []\n",
    "    fourth_list_test = []\n",
    "    fifth_list_test = []\n",
    "    for line in f:\n",
    "        row = line.strip().split('\\t')\n",
    "        if (row[3] is not None) and (row[4] is not None):\n",
    "            row_3 = eval(row[3])\n",
    "            row_4 = eval(row[4])\n",
    "            if (row_3 is not None) and (row_4 is not None):\n",
    "                if (len(row_3) == 3) and (len(row_4) == 3) and (None not in (row_3)) and (None not in (row_4)):\n",
    "                    lower_row_3 = []\n",
    "                    for i in row_3:\n",
    "                        lower_row_3.append(i.lower())\n",
    "                    lower_row_4 = []\n",
    "                    for i in row_4:\n",
    "                        lower_row_4.append(i.lower())\n",
    "                    first_list_test.append(int(row[0]))\n",
    "                    fourth_list_test.append(lower_row_3)\n",
    "                    fifth_list_test.append(lower_row_4)\n",
    "\n",
    "\n",
    "model = api.load(\"glove-twitter-50\")\n",
    "\n",
    "\n",
    "# List of words\n",
    "\n",
    "fourth_list_tensor_train = []\n",
    "fifth_list_tensor_train = []\n",
    "new_label_train = []\n",
    "\n",
    "for i in range(0, len(fourth_list_train)):\n",
    "    try:\n",
    "        vectors1 = [model[word1] for word1 in fourth_list_train[i]]\n",
    "        numpy_array1 = np.array(vectors1)\n",
    "        tensor1 = torch.tensor(numpy_array1)\n",
    "        \n",
    "\n",
    "        vectors2 = [model[word2] for word2 in fifth_list_train[i]]\n",
    "        numpy_array2 = np.array(vectors2)\n",
    "        tensor2 = torch.tensor(numpy_array2)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    fourth_list_tensor_train.append(tensor1)\n",
    "    fifth_list_tensor_train.append(tensor2)\n",
    "    new_label_train.append(float(first_list_train[i]))\n",
    "\n",
    "\n",
    "fourth_list_tensor_test = []\n",
    "fifth_list_tensor_test = []\n",
    "new_label_test = []\n",
    "\n",
    "for i in range(0, len(fourth_list_test)):\n",
    "    try:\n",
    "        vectors1 = [model[word1] for word1 in fourth_list_test[i]]\n",
    "        numpy_array1 = np.array(vectors1)\n",
    "        tensor1 = torch.tensor(numpy_array1)\n",
    "        \n",
    "\n",
    "        vectors2 = [model[word2] for word2 in fifth_list_test[i]]\n",
    "        numpy_array2 = np.array(vectors2)\n",
    "        tensor2 = torch.tensor(numpy_array2)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    fourth_list_tensor_test.append(tensor1)\n",
    "    fifth_list_tensor_test.append(tensor2)\n",
    "    new_label_test.append(float(first_list_test[i]))\n",
    "\n",
    "\n",
    "print(len(fourth_list_tensor_test))\n",
    "print(len(fifth_list_tensor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to tensors\n",
    "x1_train = torch.stack(fourth_list_tensor_train)\n",
    "x2_train = torch.stack(fifth_list_tensor_train)\n",
    "y_train = torch.tensor(new_label_train)\n",
    "\n",
    "# Create a TensorDataset from x1, x2, and y\n",
    "train_dataset = TensorDataset(x1_train, x2_train, y_train)\n",
    "\n",
    "# Convert lists to tensors\n",
    "x1_test = torch.stack(fourth_list_tensor_test)\n",
    "x2_test = torch.stack(fifth_list_tensor_test)\n",
    "y_test = torch.tensor(new_label_test)\n",
    "\n",
    "# Create a TensorDataset from x1, x2, and y\n",
    "test_dataset = TensorDataset(x1_test, x2_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create a DataLoader from the TensorDataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE CNN MODEL\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (3, 50)\n",
    "\n",
    "# Define the number of filters\n",
    "filters = 1\n",
    "\n",
    "# Define the kernel size\n",
    "kernel_size = 3\n",
    "\n",
    "# Define the number of units in the dense layer\n",
    "units = 10\n",
    "\n",
    "# Define the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(96, 50)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Pass sentence 1 through the network\n",
    "        x1 = x1.unsqueeze(1)\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool1(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool2(x1)\n",
    "        x1 = torch.flatten(x1, 1) # Flatten the output\n",
    "        x1 = self.fc1(x1)\n",
    "\n",
    "        # Pass sentence 2 through the network\n",
    "        x2 = x2.unsqueeze(1)\n",
    "        x2 = self.conv1(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.pool1(x2)\n",
    "        x2 = self.conv2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.pool2(x2)\n",
    "        #x2 = x2.view(-1, 6144) # Flatten the output\n",
    "        x2 = torch.flatten(x2, 1)\n",
    "        x2 = self.fc1(x2)\n",
    "\n",
    "        # Calculate Manhattan distance\n",
    "        d = torch.abs(x1 - x2).sum(dim=1)\n",
    "\n",
    "        # Calculate similarity score using e^-d\n",
    "        score = torch.exp(-d)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = CNN()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "def train(model, train_loader, criterion=criterion, optimizer=optimizer, n_epoch=n_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        running_loss = 0\n",
    "        for i, (x1, x2, y) in enumerate(train_loader):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(x1, x2)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6814469078179697\n",
      "0.8070671378091872\n"
     ]
    }
   ],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    for i, (x1, x2, y) in enumerate(val_loader):\n",
    "        # your code here\n",
    "        predicted = model(x1, x2)\n",
    "        for element in predicted:\n",
    "            Y_pred.append(element.item())\n",
    "        for element in y:\n",
    "            Y_true.append(element.item())\n",
    "\n",
    "    return Y_pred, Y_true\n",
    "\n",
    "y_pred, y_true = eval_model(model, val_loader)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] >= 0.5:\n",
    "        y_pred[i] = 1.0\n",
    "    else:\n",
    "        y_pred[i] = 0.0\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1score = f1_score(y_true, y_pred)\n",
    "print(acc)\n",
    "print(f1score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
